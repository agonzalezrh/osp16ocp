- copy:
    content: '#!/bin/bash

      /var/lib/container-config-scripts/pacemaker_mutex_restart_bundle.sh --lock $*
      2>&1 | logger -t certmonger'
    dest: /usr/bin/certmonger-ha-resource-refresh.sh
    mode: '0700'
    setype: certmonger_unconfined_exec_t
  name: create certificate rotation script for HA services
- file:
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - path: /etc/iscsi
    setype: svirt_sandbox_file_t
  - path: /etc/target
    setype: svirt_sandbox_file_t
  - path: /var/lib/iscsi
    setype: svirt_sandbox_file_t
- name: stat /lib/systemd/system/iscsid.socket
  register: stat_iscsid_socket
  stat: path=/lib/systemd/system/iscsid.socket
- name: Stop and disable iscsid.socket service
  service: name=iscsid.socket state=stopped enabled=no
  when: stat_iscsid_socket.stat.exists
- command: systemctl is-enabled --quiet iscsi.service
  failed_when: false
  name: Check if iscsi.service is enabled
  register: iscsi_service_enabled_result
- name: Stop iscsi.service
  service: name=iscsi.service state=stopped enabled=no
  when:
  - iscsi_service_enabled_result is changed
  - iscsi_service_enabled_result.rc == 0
- include_role:
    name: tripleo-kernel
- file:
    mode: '{{ item.mode }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/nova
    setype: svirt_sandbox_file_t
- file:
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - path: /var/lib/nova
    setype: svirt_sandbox_file_t
  - path: /var/lib/_nova_secontext
    setype: svirt_sandbox_file_t
  - path: /var/lib/nova/instances
    setype: svirt_sandbox_file_t
  - path: /var/lib/libvirt
    setype: svirt_sandbox_file_t
- mount: name=/var/lib/nova/instances src="{{nfs_share}}" fstype=nfs4 opts="_netdev,bg,{{nfs_options}},vers={{nfs_vers}},nfsvers={{nfs_vers}}"
    state=mounted
  name: Mount Nova NFS Share
  vars:
    nfs_backend_enable: false
    nfs_options: context=system_u:object_r:nfs_t:s0
    nfs_share: ''
    nfs_vers: '4'
  when: nfs_backend_enable|bool
- name: is Nova Resume Guests State On Host Boot enabled
  set_fact:
    resume_guests_state_on_host_boot_enabled: false
- block:
  - copy:
      content: '[Unit]

        Description=Suspend/Resume Running libvirt Guests

        After=network.target

        After=time-sync.target

        After=virt-guest-shutdown.target

        After=docker.service

        After=paunch-container-shutdown.service

        After=rhel-push-plugin.service

        Documentation=man:libvirtd(8)

        Documentation=https://libvirt.org


        [Service]

        EnvironmentFile=-/var/lib/config-data/puppet-generated/nova_libvirt/etc/sysconfig/libvirt-guests

        # Hack just call traditional service until we factor

        # out the code

        ExecStart=/bin/{{container_cli}} exec nova_libvirt /bin/sh -x /usr/libexec/libvirt-guests.sh
        start

        ExecStop=/bin/{{container_cli}} stop nova_compute

        ExecStop=/bin/{{container_cli}}  exec nova_libvirt /bin/sh -x /usr/libexec/libvirt-guests.sh
        stop

        Type=oneshot

        RemainAfterExit=yes

        StandardOutput=journal+console

        TimeoutStopSec=0


        [Install]

        WantedBy=multi-user.target

        '
      dest: /etc/systemd/system/libvirt-guests.service
    name: libvirt-guests unit to stop nova_compute container before shutdown VMs
  - copy:
      content: '[Unit]

        Description=Libvirt guests shutdown

        Documentation=https://libvirt.org

        '
      dest: /etc/systemd/system/virt-guest-shutdown.target
    name: Making sure virt-guest-shutdown.target is present
  - name: libvirt-guests enable VM shutdown on compute reboot/shutdown
    systemd:
      daemon_reload: true
      enabled: true
      name: libvirt-guests
  name: install libvirt-guests systemd unit file (docker)
  when:
  - resume_guests_state_on_host_boot_enabled|bool
  - container_cli == 'docker'
- block:
  - copy:
      content: '[Unit]

        Description=Suspend libvirt Guests in tripleo

        Requires=virt-guest-shutdown.target

        After=systemd-machined.service

        After=tripleo_nova_libvirt.service

        Before=tripleo_nova_compute.service

        Documentation=man:libvirtd(8)

        Documentation=https://libvirt.org


        [Service]

        EnvironmentFile=-/etc/sysconfig/libvirt-guests

        ExecStart=/bin/{{container_cli}} exec nova_libvirt /bin/rm -f /var/lib/libvirt/libvirt-guests

        ExecStop=/bin/{{container_cli}} exec nova_libvirt /bin/sh -x /usr/libexec/libvirt-guests.sh
        shutdown

        Type=oneshot

        RemainAfterExit=yes

        StandardOutput=journal+console

        TimeoutStopSec=0


        [Install]

        WantedBy=multi-user.target

        '
      dest: /etc/systemd/system/tripleo_nova_libvirt_guests.service
    name: libvirt-guests unit to stop nova_compute container before shutdown VMs
  - copy:
      content: '[Unit]

        Description=Libvirt guests shutdown

        Documentation=https://libvirt.org

        '
      dest: /etc/systemd/system/virt-guest-shutdown.target
    name: Making sure virt-guest-shutdown.target is present
  - name: tripleo_nova_libvirt_guests enable VM shutdown on compute reboot/shutdown
    systemd:
      daemon_reload: true
      enabled: true
      name: tripleo_nova_libvirt_guests
  name: install tripleo_nova_libvirt_guests systemd unit file (podman)
  when:
  - resume_guests_state_on_host_boot_enabled|bool
  - container_cli == 'podman'
- file:
    path: /etc/ceph
    state: directory
  name: ensure ceph configurations exist
- name: is Instance HA enabled
  set_fact:
    instance_ha_enabled: false
- name: enable virt_sandbox_use_netlink for healthcheck
  seboolean:
    name: virt_sandbox_use_netlink
    persistent: true
    state: true
- block:
  - file:
      path: /var/lib/nova/instanceha
      state: directory
    name: prepare Instance HA script directory
  - copy:
      content: "#!/usr/libexec/platform-python\n\nimport os\nimport sys\nimport time\n\
        import inspect\nimport logging\nimport argparse\nimport oslo_config.cfg\n\
        import requests.exceptions\n\ndef is_forced_down(connection, hostname):\n\
        \    services = connection.services.list(host=hostname, binary=\"nova-compute\"\
        )\n    for service in services:\n        if service.forced_down:\n       \
        \     return True\n    return False\n\ndef evacuations_done(connection, hostname):\n\
        \    # Get a list of migrations.\n    #  :param host: (optional) filter migrations\
        \ by host name.\n    #  :param status: (optional) filter migrations by status.\n\
        \    #  :param cell_name: (optional) filter migrations for a cell.\n    #\n\
        \    migrations = connection.migrations.list(host=hostname)\n\n    print(\"\
        Checking %d migrations\" % len(migrations))\n    for migration in migrations:\n\
        \        # print migration.to_dict()\n        #\n        # {\n        # u'status':\
        \ u'error',\n        # u'dest_host': None,\n        # u'new_instance_type_id':\
        \ 2,\n        # u'old_instance_type_id': 2,\n        # u'updated_at': u'2018-04-22T20:55:29.000000',\n\
        \        # u'dest_compute':\n        #   u'overcloud-novacompute-2.localdomain',\n\
        \        # u'migration_type': u'live-migration',\n        # u'source_node':\n\
        \        #   u'overcloud-novacompute-0.localdomain',\n        # u'id': 8,\n\
        \        # u'created_at': u'2018-04-22T20:52:58.000000',\n        # u'instance_uuid':\n\
        \        #   u'd1c82ce8-3dc5-48db-b59f-854b3b984ef1',\n        # u'dest_node':\n\
        \        #   u'overcloud-novacompute-2.localdomain',\n        # u'source_compute':\n\
        \        #   u'overcloud-novacompute-0.localdomain'\n        # }\n       \
        \ # Acceptable: done, completed, failed\n        if migration.status in [\"\
        running\", \"accepted\", \"pre-migrating\"]:\n            return False\n \
        \   return True\n\ndef safe_to_start(connection, hostname):\n    if is_forced_down(connection,\
        \ hostname):\n        print(\"Waiting for fence-down flag to be cleared\"\
        )\n        return False\n    if not evacuations_done(connection, hostname):\n\
        \        print(\"Waiting for evacuations to complete or fail\")\n        return\
        \ False\n    return True\n\ndef create_nova_connection(options):\n    try:\n\
        \        from novaclient import client\n        from novaclient.exceptions\
        \ import NotAcceptable\n    except ImportError:\n        print(\"Nova not\
        \ found or not accessible\")\n        sys.exit(1)\n\n    from keystoneauth1\
        \ import loading\n    from keystoneauth1 import session\n    from keystoneclient\
        \ import discover\n\n    # Prefer the oldest and strip the leading 'v'\n \
        \   keystone_versions = discover.available_versions(options[\"auth_url\"][0])\n\
        \    keystone_version = keystone_versions[0]['id'][1:]\n    kwargs = dict(\n\
        \        auth_url=options[\"auth_url\"][0],\n        username=options[\"username\"\
        ][0],\n        password=options[\"password\"][0]\n        )\n\n    if discover.version_match(\"\
        2\", keystone_version):\n        kwargs[\"tenant_name\"] = options[\"tenant_name\"\
        ][0]\n\n    elif discover.version_match(\"3\", keystone_version):\n      \
        \  kwargs[\"project_name\"] = options[\"project_name\"][0]\n        kwargs[\"\
        user_domain_name\"] = options[\"user_domain_name\"][0]\n        kwargs[\"\
        project_domain_name\"] = options[\"project_domain_name\"][0]\n\n    loader\
        \ = loading.get_plugin_loader('password')\n    keystone_auth = loader.load_from_options(**kwargs)\n\
        \    keystone_session = session.Session(auth=keystone_auth, verify=(not options[\"\
        insecure\"]))\n\n    nova_endpoint_type = 'internalURL'\n    # We default\
        \ to internalURL but we allow this to be overridden via\n    # the [placement]/os_interface\
        \ key.\n    if 'os_interface' in options and len(options[\"os_interface\"\
        ]) == 1:\n        nova_endpoint_type = options[\"os_interface\"][0]\n    #\
        \ Via https://review.opendev.org/#/c/492247/ os_interface has been deprecatd\
        \ in queens\n    # and we need to use 'valid_interfaces' which is a:\n   \
        \ # \"List of interfaces, in order of preference, for endpoint URL. (list\
        \ value)\"\n    # Since it is not explicitely set in nova.conf we still keep\
        \ the check for os_interface\n    elif 'valid_interfaces' in options and len(options[\"\
        valid_interfaces\"]) >= 1:\n        nova_endpoint_type = options[\"valid_interfaces\"\
        ][0]\n\n    # This mimicks the code in novaclient/shell.py\n    if nova_endpoint_type\
        \ in ['internal', 'public', 'admin']:\n        nova_endpoint_type += 'URL'\n\
        \n    if 'region_name' in options:\n        region = options['region_name'][0]\n\
        \    elif 'os_region_name' in options:\n        region = options['os_region_name'][0]\n\
        \    else: # We actually try to make a client call even with an empty region\n\
        \        region = None\n    nova_versions = [ \"2.23\", \"2\" ]\n    for version\
        \ in nova_versions:\n        clientargs = inspect.getargspec(client.Client).varargs\n\
        \        # Some versions of Openstack prior to Ocata only\n        # supported\
        \ positional arguments for username,\n        # password, and tenant.\n  \
        \      #\n        # Versions since Ocata only support named arguments.\n \
        \       #\n        # So we need to use introspection to figure out how to\n\
        \        # create a Nova client.\n        #\n        # Happy days\n      \
        \  #\n        if clientargs:\n            # OSP < Ocata\n            # ArgSpec(args=['version',\
        \ 'username', 'password', 'project_id', 'auth_url'],\n            #      \
        \   varargs=None,\n            #         keywords='kwargs', defaults=(None,\
        \ None, None, None))\n            nova = client.Client(version,\n        \
        \                         None, # User\n                                 None,\
        \ # Password\n                                 None, # Tenant\n          \
        \                       None, # Auth URL\n                               \
        \  insecure=options[\"insecure\"],\n                                 region_name=region,\n\
        \                                 session=keystone_session, auth=keystone_auth,\n\
        \                                 http_log_debug=\"verbose\" in options,\n\
        \                                 endpoint_type=nova_endpoint_type)\n    \
        \    else:\n            # OSP >= Ocata\n            # ArgSpec(args=['version'],\
        \ varargs='args', keywords='kwargs', defaults=None)\n            nova = client.Client(version,\n\
        \                                 region_name=region,\n                  \
        \               session=keystone_session, auth=keystone_auth,\n          \
        \                       http_log_debug=\"verbose\" in options,\n         \
        \                        endpoint_type=nova_endpoint_type)\n\n        try:\n\
        \            nova.hypervisors.list()\n            return nova\n\n        except\
        \ NotAcceptable as e:\n            logging.warning(e)\n\n        except Exception\
        \ as e:\n            logging.warning(\"Nova connection failed. %s: %s\" %\
        \ (e.__class__.__name__, e))\n\n    print(\"Couldn't obtain a supported connection\
        \ to nova, tried: %s\\n\" % repr(nova_versions))\n    return None\n\n\nparser\
        \ = argparse.ArgumentParser(description='Process some integers.')\nparser.add_argument('--config-file',\
        \ dest='nova_config', action='store',\n                    default=\"/etc/nova/nova.conf\"\
        ,\n                    help='path to nova configuration (default: /etc/nova/nova.conf)')\n\
        parser.add_argument('--nova-binary', dest='nova_binary', action='store',\n\
        \                    default=\"/usr/bin/nova-compute\",\n                \
        \    help='path to nova compute binary (default: /usr/bin/nova-compute)')\n\
        parser.add_argument('--enable-file', dest='enable_file', action='store',\n\
        \                    default=\"/var/lib/nova/instanceha/enabled\",\n     \
        \               help='file exists if instance HA is enabled on this host '\\\
        \n                    '(default: /var/lib/nova/instanceha/enabled)')\n\n\n\
        sections = {}\n(args, remaining) = parser.parse_known_args(sys.argv)\n\nconfig\
        \ = oslo_config.cfg.ConfigParser(args.nova_config, sections)\nconfig.parse()\n\
        config.sections[\"placement\"][\"insecure\"] = 0\nconfig.sections[\"placement\"\
        ][\"verbose\"] = 1\n\nif os.path.isfile(args.enable_file):\n    connection\
        \ = None\n    while not connection:\n        # Loop in case the control plane\
        \ is recovering when we run\n        connection = create_nova_connection(config.sections[\"\
        placement\"])\n        if not connection:\n            time.sleep(10)\n\n\
        \    while not safe_to_start(connection, config.sections[\"DEFAULT\"][\"host\"\
        ][0]):\n        time.sleep(10)\n\nreal_args = [args.nova_binary, '--config-file',\
        \ args.nova_config]\nreal_args.extend(remaining[1:])\nos.execv(args.nova_binary,\
        \ real_args)\n"
      dest: /var/lib/nova/instanceha/check-run-nova-compute
      mode: 493
    name: install Instance HA script that runs nova-compute
  - command: hiera -c /etc/puppet/hiera.yaml compute_instanceha_short_node_names
    name: Get list of instance HA compute nodes
    register: iha_nodes
  - file: path=/var/lib/nova/instanceha/enabled state=touch
    name: If instance HA is enabled on the node activate the evacuation completed
      check
    when: iha_nodes.stdout|lower is search('"'+ansible_facts['hostname']|lower+'"')
  name: install Instance HA recovery script
  when: instance_ha_enabled|bool
- name: Is irqbalance enabled
  set_fact:
    compute_irqbalance_disabled: false
- name: disable irqbalance service on compute
  service:
    enabled: false
    name: irqbalance.service
    state: stopped
  when: compute_irqbalance_disabled|bool
- file:
    mode: '{{ item.mode }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/libvirt
    setype: svirt_sandbox_file_t
- file:
    path: '{{ item.path }}'
    setype: '{{ item.setype | default(omit) }}'
    state: directory
  name: create libvirt persistent data directories
  with_items:
  - path: /etc/libvirt
    setype: svirt_sandbox_file_t
  - path: /etc/libvirt/secrets
    setype: svirt_sandbox_file_t
  - path: /etc/libvirt/qemu
    setype: svirt_sandbox_file_t
  - path: /var/lib/libvirt
    setype: svirt_sandbox_file_t
  - path: /var/cache/libvirt
  - path: /var/lib/nova
    setype: svirt_sandbox_file_t
  - path: /var/run/libvirt
    setype: virt_var_run_t
  - path: /var/log/libvirt
    setype: svirt_sandbox_file_t
  - path: /var/log/libvirt/qemu
    setype: svirt_sandbox_file_t
- group:
    gid: 107
    name: qemu
    state: present
  name: ensure qemu group is present on the host
- name: ensure qemu user is present on the host
  user:
    comment: qemu user
    group: qemu
    name: qemu
    shell: /sbin/nologin
    state: present
    uid: 107
- file:
    group: qemu
    owner: qemu
    path: /var/lib/vhost_sockets
    setype: virt_cache_t
    seuser: system_u
    state: directory
  name: create directory for vhost-user sockets with qemu ownership
- check_mode: false
  command: /usr/bin/rpm -q libvirt-daemon
  failed_when: false
  name: check if libvirt is installed
  register: libvirt_installed
- name: make sure libvirt services are disabled and masked
  service:
    daemon_reload: true
    enabled: false
    masked: true
    name: '{{ item }}'
    state: stopped
  when: libvirt_installed.rc == 0
  with_items:
  - libvirtd.service
  - virtlogd.socket
- copy:
    content: 'd /var/run/libvirt 0755 root root - -

      '
    dest: /etc/tmpfiles.d/var-run-libvirt.conf
  name: ensure /var/run/libvirt is present upon reboot
- file:
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  loop:
  - path: /var/run/libvirt
    setype: virt_var_run_t
  name: Create libvirt persistent data directories
- name: allow logrotate to read inside containers
  seboolean:
    name: logrotate_read_inside_containers
    persistent: true
    state: true
- block:
  - name: Set login facts
    set_fact:
      container_default_pids_limit: 4096
      container_events_logger_mechanism: journald
      container_registry_insecure_registries: []
      container_registry_login: false
      container_registry_logins: {}
      container_registry_logins_json: {}
  - name: Convert logins json to dict
    set_fact:
      container_registry_logins: '{{ container_registry_logins_json | from_json }}'
    when:
    - container_registry_logins_json is string
    - container_registry_login | bool
    - (container_registry_logins_json | length) > 0
  - name: Set registry logins
    set_fact:
      container_registry_logins: '{{ container_registry_logins_json }}'
    when:
    - container_registry_logins_json is mapping
    - container_registry_login | bool
    - (container_registry_logins_json | length) > 0
  - include_role:
      name: tripleo-podman
      tasks_from: tripleo_podman_install.yml
    name: Run podman install
    vars:
      tripleo_container_default_pids_limit: '{{ container_default_pids_limit }}'
      tripleo_container_events_logger_mechanism: '{{ container_events_logger_mechanism
        }}'
      tripleo_container_registry_insecure_registries: '{{ container_registry_insecure_registries
        }}'
  - include_role:
      name: tripleo-podman
      tasks_from: tripleo_podman_login.yml
    name: Run podman login
    vars:
      tripleo_container_registry_login: '{{ container_registry_login | bool }}'
      tripleo_container_registry_logins: '{{ container_registry_logins }}'
  name: Install and configure Podman
- copy:
    content: 'This file makes paunch generate additional systemd

      dependencies for containers that have special

      start/stop ordering constraints. It ensures that

      those constraints are enforced on reboot/shutdown.

      '
    dest: /etc/sysconfig/podman_drop_in
  name: Configure paunch to generate systemd drop-in dependencies
- become: true
  failed_when: false
  name: Check for NTP service
  register: ntp_service_check
  shell: systemctl is-active ntpd.service || systemctl is-enabled ntpd.service
- name: Disable NTP before configuring Chrony
  service:
    enabled: false
    name: ntpd
    state: stopped
  when:
  - ntp_service_check.rc is defined
  - ntp_service_check.rc == 0
- include_role:
    name: chrony
  name: Install, Configure and Run Chrony
- meta: flush_handlers
  name: Ensure chrony has been restarted
- command: chronyc makestep
  name: Ensure system is NTP time synced
- name: Set timezone fact
  set_fact:
    timezone: UTC
- name: Set timezone to {{ timezone | default('UTC') }}
  register: timezone_result
  timezone:
    name: '{{ timezone }}'
- failed_when: false
  name: Restart services
  service:
    name: '{{ item }}'
    state: restarted
  when:
  - timezone_result.changed
  with_items:
  - rsyslog
  - crond
- include_role:
    name: tuned
- file:
    mode: '{{ item.mode|default(omit) }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/openvswitch
    setype: svirt_sandbox_file_t
  - path: /var/lib/openvswitch/ovn
    setype: svirt_sandbox_file_t
- copy:
    content: "#!/bin/bash\n# Cleanup neutron OVS bridges. To be called on startup\
      \ to avoid\n# \"difficult-to-debug\" issues with partially configured resources.\n\
      \nNEUTRON_OVS_CONF=/var/lib/config-data/puppet-generated/neutron/etc/neutron/plugins/ml2/openvswitch_agent.ini\n\
      \nif [ -e ${NEUTRON_OVS_CONF} ];\nthen\n    INT_BRIDGE=`crudini --get ${NEUTRON_OVS_CONF}\
      \ ovs integration_bridge`\n    TUN_BRIDGE=`crudini --get ${NEUTRON_OVS_CONF}\
      \ ovs tunnel_bridge`\nfi\n\nfor port in `ovs-vsctl list-ports ${INT_BRIDGE:-\"\
      br-int\"}`;\ndo\n    skip_cleanup=`ovs-vsctl --if-exists get Interface $port\
      \ external_ids:skip_cleanup`\n    if ! [[ \"x$skip_cleanup\" == \"x\\\"true\\\
      \"\" ]];\n    then\n        ovs-vsctl del-port ${INT_BRIDGE:-\"br-int\"} $port\n\
      \    fi\ndone\n\novs-vsctl --if-exists del-br ${TUN_BRIDGE:-\"br-tun\"}\n\n\
      # Clean up trunk port bridges\nfor br in $(ovs-vsctl list-br | egrep 'tbr-[0-9a-f\\\
      -]+'); do\n    ovs-vsctl --if-exists del-br $br\ndone\n"
    dest: /usr/libexec/neutron-cleanup
    force: true
    mode: '0755'
  name: Copy in cleanup script
- copy:
    content: '[Unit]

      Description=Neutron cleanup on startup

      After=openvswitch.service network.target

      Before=tripleo_neutron_ovs_agent.service tripleo_neutron_dhcp.service tripleo_neutron_l3_agent.service
      tripleo_nova_compute.service

      RefuseManualStop=yes


      [Service]

      Type=oneshot

      ExecStart=/usr/libexec/neutron-cleanup


      [Install]

      WantedBy=multi-user.target

      '
    dest: /usr/lib/systemd/system/neutron-cleanup.service
    force: true
  name: Copy in cleanup service
- name: Enabling the cleanup service
  service:
    enabled: true
    name: neutron-cleanup
- file:
    mode: '{{ item.mode }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/neutron
    setype: svirt_sandbox_file_t
- command: ip netns add ns_temp
  failed_when: false
  name: create /run/netns with temp namespace
  register: ipnetns_add_result
- command: ip netns delete ns_temp
  failed_when: false
  name: remove temp namespace
  when:
  - ipnetns_add_result.rc is defined
  - ipnetns_add_result.rc == 0
- file:
    path: /var/lib/neutron
    setype: svirt_sandbox_file_t
    state: directory
  name: create /var/lib/neutron
- name: set conditions
  set_fact:
    debug_enabled: false
    docker_additional_sockets:
    - /var/lib/openstack/docker.sock
    haproxy_wrapper_enabled: true
- file:
    path: /var/lib/neutron/kill_scripts
    state: directory
  name: create kill_scripts directory within /var/lib/neutron
- copy:
    content: "#!/bin/bash\n{% if debug_enabled|bool -%}\nset -x\n{% endif -%}\nadd_date()\
      \ {\n  echo \"$(date) $@\"\n}\n\n# Set up script logging for debugging purpose.\n\
      # It will be taken care of by logrotate since there is the .log\n# suffix.\n\
      exec 3>&1 4>&2\ntrap 'exec 2>&4 1>&3' 0 1 2 3\nexec 1>>/var/log/neutron/kill-script.log\
      \ 2>&1\n\nSIG=$1\nPID=$2\nNETNS=$(ip netns identify ${PID})\n\n{% if container_cli\
      \ == 'podman' %}\nif [ \"x${NETNS}\" == \"x\" ]; then\n  CLI=\"nsenter --all\
      \ --preserve-credentials -t 1 podman\"\n  SIG=9\nelse\n  CLI=\"nsenter --net=/run/netns/${NETNS}\
      \ --preserve-credentials -m -t 1 podman\"\nfi\n{% elif container_cli == 'docker'\
      \ %}\n{% if docker_additional_sockets and docker_additional_sockets|length >\
      \ 0-%}\nexport DOCKER_HOST=unix://{{ docker_additional_sockets[0] }}\n{% endif\
      \ -%}\nCLI='docker'\n{% else %}\nCLI='echo noop'\n{% endif %}\n\nkill_container()\
      \ {\n  add_date \"Stopping container $1 ($2)\"\n  $CLI stop $2\n  add_date \"\
      Deleting container $1 ($2)\"\n  $CLI rm $2\n}\n\nsignal_container() {\n  SIGNAL=$3\n\
      \  if [ -z \"$SIGNAL\" ]; then\n      SIGNAL=\"HUP\"\n  fi\n  add_date \"Sending\
      \ signal '$SIGNAL' to $1 ($2)\"\n  $CLI kill --signal $SIGNAL $2\n}\n\n{% raw\
      \ -%}\nif [ -f /proc/$PID/cgroup ]; then\n  # Get container ID based on process\
      \ cgroups\n  CT_ID=$(awk 'BEGIN {FS=\"[-.]\"} /name=/{print $3}' /proc/$PID/cgroup)\n\
      \  CT_NAME=$($CLI inspect -f '{{.Name}}' $CT_ID)\n\n  case $SIG in\n    HUP)\n\
      \      signal_container $CT_NAME $CT_ID\n      ;;\n    9)\n      kill_container\
      \ $CT_NAME $CT_ID\n      ;;\n    15)\n      signal_container $CT_NAME $CT_ID\
      \ 15\n      ;;\n    *)\n      add_date \"Unknown action ${SIG} for ${CT_NAME}\
      \ ${CT_ID}\"\n      exit 1\n      ;;\n  esac\n\nelse\n  add_date \"No such PID:\
      \ ${PID}\"\n  exit 1\nfi\n{% endraw %}\n"
    dest: /var/lib/neutron/kill_scripts/haproxy-kill
    mode: 493
  name: create haproxy kill script
  when: haproxy_wrapper_enabled|bool
